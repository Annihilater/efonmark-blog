---
title: 简单使用了下scrapy爬虫工具
date: 2019-12-18
tags: 
categories: 极客工具
---

> **一番码客 : 挖掘你关心的亮点。**
> **http://www.efonmark.com**

本文目录：

[TOC]

![image-20191217222038262](2019-12-18-简单使用了下scrapy爬虫工具/image-20191217222038262.png)

<!-- more -->

## 前言

前天一番写了《用爬虫看看我们工作的”前途“》，里面收集了52job上在深圳的”前端“和”区块链“两个关键字的职位信息。

其实是因为一番想了解下相关职位在市场上的职位要求，而想到用爬虫去爬取相关信息，显然没有做完。因为一番在尝试用一个爬虫框架——scrapy，之前没有接触过，所以只是简单运用了下，几乎就是搭建环境和初步认识。

这篇文章就来介绍一下如何简单的使用scrapy爬虫工具来爬取一些简单的网页信息，获得格式化的数据。

## 开发环境

本文的开发环境，也是使用这个scrapy的以来环境如下。

* 操作系统：windows10。
* python3.7 + pip3。
* IDE：Visual Studio Code。

## 创建项目

在powershell简单执行如下几条命令，便可以实现项目的创建。

* 安装scrapy开发环境。

    其中`-i https://pypi.tuna.tsinghua.edu.cn/simple`是重定向下载源，这里定向的国内清华的源，会加快下载速度，否则奇慢难忍。

    ```powershell
    pip3 install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple
    ```

* 创建scrapy工程

    ```powershell
    scrapy startproject zhaopin
    ```

* 创建爬虫实例

    * `51job`是实例名，会在spider目录下创建一个同名`.py`文件。因为文件名不能以数字开头，会默认加个`a`的前缀。
    * `www.51job.com`，是爬虫允许请求的域名地址。

    ```powershell
    scrapy genspider 51job www.51job.com
    ```

* 执行爬虫

    * `51job`就是我们刚刚创建的爬虫实例。
    * 也就是说我们可以用`scrapy genspider`命令创建任意多个爬虫实例，然后用`scrapy crawl`命令来执行任意一个存在的实例。

    ```powershell
    scrapy crawl 51job
    ```

## 目录结构

```shell
├── scrapy.cfg			// 项目的配置文件。
└── zhaopin				// 该项目的python模块。之后您将在此加入代码。
    ├── items.py		// 项目中的item文件。
    ├── middlewares.py	// 爬虫中间件。
    ├── pipelines.py	// 项目中的pipelines文件。
    ├── settings.py		// 项目的设置文件。
    └── spiders			// 放置spider代码的目录。
```

## 基本流程

查看`a51job.py`文件。

```python
# -*- coding: utf-8 -*-
import scrapy


class A51jobSpider(scrapy.Spider):
    name = '51job'
    allowed_domains = ['www.51job.com']
    start_urls = ['http://www.51job.com/']

    def parse(self, response):
        pass
```

当我们执行`scrapy crawl 51job`时：

* scrapy为Spider的 `start_urls` 属性中的每个URL创建了 `scrapy.Request`对象，并将 `parse` 方法作为**回调函数(callback)**赋值给了Request。
* Request对象经过调度，执行生成 `scrapy.http.Response`对象并送回给spider `parse()`方法。
* 也即，`parse`函数中`respose`便是`start_urls`中请求回来的结果。

## 总结

好了，以上便可以完整的看到从scrapy项目创建到运行的各个步骤，以及运行时的基本运作流程。

便算是可以通过这个简单的步骤，可以实现用scrapy抓取想要下载页面的内容了。

随着一番实际使用的深入，后面还会继续写一些scrapy的深入文章。

> 一番雾语：python几步实现获取一个的网页的内容。

------------------

> **免费知识星球： [一番码客-积累交流](http://www.efonmark.com/efonmark-blog/readme/zhishixingqiu1.png)**
> **微信公众号：[一番码客](http://www.efonmark.com/efonmark-blog/readme/guanzhu_1.jpg)**
> **微信：[Efon-fighting](http://www.efonmark.com/efonmark-blog/readme/weixin.jpg)**
> **网站： [http://www.efonmark.com](http://www.efonmark.com)**